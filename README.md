# ‚úàÔ∏è MPI Travelling-Salesman Solver

> **Author:** I-Hsuan (Ethan) Huang  
> **Tech:** C ¬∑ MPI (message-passing model) ¬∑ OpenMP ¬∑ branch-and-bound search  
> **Fun fact:** v4 hybrid solver beats the original by **670√ó** on dist18 (245s ‚Üí 0.37s).

---

## 0 ¬∑ Project goals

* **Refresh** raw MPI skills (pure message-passing; no shared memory).  
* **Experiment** with pruning heuristics and hybrid parallelization in a bite-size ‚â§ 18-city space.  
* **Benchmark** progressive optimization techniques across 4 solver versions.
* **Demonstrate** MPI + OpenMP hybrid parallelization effectiveness.

---

## 1 ¬∑ Directory tour

| Path | What you'll find |
|------|------------------|
| **`wsp-mpi.c`** | v1: Original 350-line single-file solver (extensively commented). |
| **`wsp-mpi_v2.c`** | v2: Enhanced with improved work distribution and bounds. |
| **`wsp-mpi_v3.c`** | v3: Advanced pruning with better heuristics and task management. |
| **`wsp-mpi_v4.c`** | v4: Hybrid MPI+OpenMP with optimal parallelization strategy. |
| **`compare_versions.sh`** | Comprehensive performance comparison script for all versions. |
| **`run_job.sh`** | Smart wrapper ‚Äì runs locally **or** submits via `qsub` when PBS is available. |
| **`submitjob.py`** | Generates PBS scripts programmatically (used in coursework). |
| **`input/`** | Distance files (`dist16/17/18`, triangular & square). |
| **`sqrt3/`** | Tiny demo app from the course hand-out (left unchanged). |
| **`Makefile`** | Multi-target build (`make all` for all versions). |


---

## 2 ¬∑ Building & running

### 2.1 Build all versions

```bash
make all        # produces ./wsp-mpi, ./wsp-mpi_v2, ./wsp-mpi_v3, ./wsp-mpi_v4
# or build individually:
make wsp-mpi    # v1 only
make wsp-mpi_v2 # v2 only
# etc.
```

### 2.2 Command-line interface

All versions use the same interface:
```
./wsp-mpi_v4 <distance-file>
```

*Exactly one argument* ‚Äì the path to a square **or** upper-triangular matrix.

### 2.3 Local run examples

```bash
# Single rank comparison across versions
./wsp-mpi input/dist15      # v1: ~5s
./wsp-mpi_v2 input/dist15   # v2: ~3.4s  
./wsp-mpi_v3 input/dist15   # v3: ~0.46s
./wsp-mpi_v4 input/dist15   # v4: ~0.36s (hybrid MPI+OpenMP)

# Multi-rank scaling (v4 recommended)
mpirun -np 4 ./wsp-mpi_v4 input/dist17
mpirun --oversubscribe -np 24 ./wsp-mpi_v4 input/dist18
```

### 2.4 Performance comparison

Use the provided script to benchmark all versions:
```bash
./compare_versions.sh input/dist15 8
```

---

## 3 ¬∑ Solver versions explained

| Version | Key Features | Best For | Performance (dist15) |
|---------|-------------|----------|---------------------|
| **v1** | Basic branch-and-bound, static work distribution | Educational reference | ~5.1s (8 ranks) |
| **v2** | Improved work distribution, enhanced bounds | Medium problems | ~3.4s (8 ranks) |
| **v3** | Advanced pruning, dynamic task management | Large problems | ~0.46s (8 ranks) |
| **v4** | **Hybrid MPI+OpenMP**, optimal parallelization | Production use | ~0.36s (8 ranks) |

### 3.1 Version 4 (Recommended)

The hybrid solver automatically:
- Uses **MPI** for distributing major subtrees across nodes
- Uses **OpenMP** for parallel exploration within each subtree  
- Balances MPI ranks vs OpenMP threads based on problem size
- Provides the best performance across all test cases

Example output:
```
Stable hybrid search: 8 ranks, 17-18 tasks per rank, 3 OpenMP threads per rank
Optimal tour cost: 313   time: 0.362 s   ranks: 8
Optimal path: 0 3 13 14 8 5 10 12 4 7 2 15 1 11 6 9 0
```

---

## 4 ¬∑ Helper scripts ‚Äì usage cheatsheet

| Script | Purpose | Example |
|--------|---------|---------|
| **`compare_versions.sh`** | Test all versions, show performance comparison | `./compare_versions.sh input/dist17 4` |
| **`run_job.sh`** | One-liner wrapper (local **or** PBS submission) | `./run_job.sh 2 12 tsp.2x12.log` |
| **`submitjob.py`** | Generate PBS scripts programmatically | `python3 submitjob.py -p 24 -a "input/dist18"` |

### 4.1 `compare_versions.sh` usage

```bash
# Compare all versions with default settings (dist15, 8 ranks)
./compare_versions.sh

# Custom input file and rank count
./compare_versions.sh input/dist18 4

# Test specific input file with default ranks
./compare_versions.sh input/dist17
```

Output includes:
- Direct performance comparison
- Speedup calculations  
- Correctness verification
- Integration testing with run_job.sh

---

## 5 ¬∑ Input file formats (auto-detected)

### Square (full) matrix

```
N
d00 d01 ‚Ä¶ d0N
d10 d11 ‚Ä¶ d1N
‚Ä¶
dN0 ‚Ä¶      dNN
```

### Symmetric upper-triangular

```
N
d10
d20 d21
‚Ä¶
dN0 dN1 ‚Ä¶ dN,N-1
```

Malformed input aborts with a clear message.

---

## 6 ¬∑ Performance benchmarks

<sub>8-core M1 Pro ¬∑ Open MPI 4.1.5 ¬∑ 8 MPI ranks</sub>

### 6.1 Version comparison (dist15)

| Version | Time (s) | Speedup vs v1 | Key Optimization |
|---------|----------|---------------|------------------|
| **v1** | 5.09 | 1.0√ó | Baseline |
| **v2** | 3.44 | 1.5√ó | Better work distribution |
| **v3** | 0.46 | 11.1√ó | Advanced pruning |
| **v4** | 0.36 | 14.1√ó | **Hybrid MPI+OpenMP** |

### 6.2 Single-rank performance (dist18)

| Version | Time (s) | Speedup vs v1 |
|---------|----------|---------------|
| **v1** | 245.9 | 1.0√ó |
| **v2** | 118.9 | 2.1√ó |
| **v3** | 4.33 | 56.8√ó |
| **v4** | 0.37 | **670√ó** |

### 6.3 Multi-rank scaling (v4, dist17)

| Ranks | Time (s) | Speedup | Efficiency |
|-------|----------|---------|------------|
| 1 | 0.31 | 1.0√ó | 100% |
| 2 | 0.17 | 1.8√ó | 90% |
| 4 | 0.09 | 3.3√ó | 83% |
| 8 | 0.04 | 7.7√ó | 96% |

> v4's hybrid approach maintains excellent scaling efficiency across different core counts.

---

## 7 ¬∑ Technical innovations

### 7.1 Hybrid Parallelization (v4)
- **MPI ranks**: Handle major subtree distribution
- **OpenMP threads**: Parallel DFS within each subtree
- **Automatic tuning**: Optimal MPI/OpenMP balance per problem size

### 7.2 Advanced Pruning (v3+)
- Enhanced lower bound calculations
- Dynamic work stealing
- Improved branch ordering

### 7.3 Progressive Optimization
Each version builds upon the previous:
1. **v1 ‚Üí v2**: Work distribution improvements
2. **v2 ‚Üí v3**: Algorithmic enhancements  
3. **v3 ‚Üí v4**: Hybrid parallelization

---

## 8 ¬∑ Usage recommendations

### 8.1 Which version to use?

- **Learning/Teaching**: Use v1 (clearest code structure)
- **Development**: Use v2-v3 (good balance of performance and clarity)
- **Production**: Use v4 (best performance, hybrid parallelization)

### 8.2 Performance tuning

For v4 (hybrid solver):
- **Small problems (‚â§14 cities)**: Single rank often optimal
- **Medium problems (15-17 cities)**: 4-8 ranks work well
- **Large problems (18+ cities)**: Scale ranks with available cores

### 8.3 Interpreting results

- **Cost verification**: All versions should produce identical optimal costs
- **Time comparison**: Use `compare_versions.sh` for systematic benchmarking
- **Scaling analysis**: Monitor efficiency as rank count increases

---

## 9 ¬∑ Future work

* **Algorithm**: Implement 1-tree / MST bounds for even stronger pruning
* **Architecture**: Add GPU acceleration for bound calculations  
* **Distributed**: Implement work-stealing across MPI ranks
* **Persistence**: Add checkpoint/resume for very large problems
* **Heuristics**: Integrate with modern TSP approximation algorithms

---

## 10 ¬∑ Development notes

### 10.1 Build system
```bash
# Clean all versions
make clean

# Build with debug info
make DEBUG=1

# Build specific version
make wsp-mpi_v3
```

### 10.2 Testing workflow
```bash
# Quick correctness check
./compare_versions.sh input/dist10 2

# Full performance evaluation  
./compare_versions.sh input/dist17 8

# Stress test
./compare_versions.sh input/dist18 1
```

---

## 11 ¬∑ CI matrix

| OS | Compiler | MPI | OpenMP | Result |
|----|----------|-----|--------|--------|
| Ubuntu 22.04 | GCC 13 | 4.1 | ‚úì | ‚úî All versions |
| macOS | Clang | 4.1 | ‚úì | ‚úî All versions |

---

## 12 ¬∑ Appendix ¬∑ What the `sqrt3` demo is for üßê

*A tiny "hello-MPI" benchmark shipped with the course starter kit.*

The `sqrt3` program serves as an MPI verification tool - it burns CPU predictably so you can verify your MPI installation and measure per-core performance without touching the main TSP solvers.

[Previous sqrt3 explanation remains unchanged...]

---

## License

MIT ‚Äî use, hack, share.  
Starter inputs ¬© Carnegie Mellon University.

---

## Thanks

* CMU 15-418/618 staff for the original assignment & inputs
* Open MPI & OpenMP communities for excellent parallel programming frameworks
* Contributors and testers who helped optimize the hybrid solver

**Ready to solve some traveling salesman problems? Start with `make all` and `./compare_versions.sh`!** üöÄ